{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70de0eb-dcbd-4f31-b5fd-22ce7f8a0d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44eec493-24dc-4bda-a400-a90e05fb1def",
   "metadata": {},
   "source": [
    "# I. Password Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "877a322c-b3b8-4d42-94e8-fe52384e7fac",
   "metadata": {},
   "source": [
    "# Step 1: Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8b8c0b-832f-4379-9b75-d507d3dff5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "rockyou_path = \"rockyou.txt\"  # Update this if your dataset is in a different location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d9ccd7-17e0-4498-91ae-7c2c4adfc21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the dataset\n",
    "with open(rockyou_path, encoding=\"latin-1\") as f:\n",
    "    passwords = f.read().splitlines()\n",
    "\n",
    "print(f\"Total passwords loaded: {len(passwords)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73085679-b23f-4641-ba4d-bd7ea3c4d0e7",
   "metadata": {},
   "source": [
    "# Step 2: Clean the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d953f8d5-319c-4356-98ea-66ae74a7189e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicates\n",
    "passwords = list(set(passwords))\n",
    "print(f\"Total passwords after removing duplicates: {len(passwords)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ddeab1-88ed-4430-b01a-907b0a40df81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove passwords shorter than 4 characters\n",
    "passwords = [pwd for pwd in passwords if len(pwd) >= 4]\n",
    "print(f\"Total passwords after removing short ones: {len(passwords)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e496ca-6305-4cda-b207-2780bf8a5975",
   "metadata": {},
   "source": [
    "# Step 3: Label passwords based on frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2799ee34-b3a0-49fb-b456-af6d178b607d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count password occurrences\n",
    "password_counts = Counter(passwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ecc60fd-cf76-4606-8f7d-f7f2af60e0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort passwords by frequency\n",
    "sorted_passwords = sorted(password_counts.items(), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "996d2928-6aec-4cd9-8518-2b9ab30eba2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define thresholds for Weak, Moderate, and Strong\n",
    "weak_threshold = 100000  # Top 100K most common passwords\n",
    "moderate_threshold = 1000000  # Next 900K passwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace56833-6a28-4418-ad9b-c95694ed9cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign labels\n",
    "labeled_passwords = []\n",
    "for i, (password, count) in enumerate(sorted_passwords):\n",
    "    if i < weak_threshold:\n",
    "        label = \"Weak\"\n",
    "    elif i < moderate_threshold:\n",
    "        label = \"Moderate\"\n",
    "    else:\n",
    "        label = \"Strong\"\n",
    "    labeled_passwords.append((password, label))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc6c4fa-26bb-4cf9-bbf0-801e0e92b8d7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(labeled_passwords, columns=[\"Password\", \"Strength\"])\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19786bed-e64b-4259-b48f-51beb588599d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save preprocessed dataset\n",
    "df.to_csv(\"preprocessed_passwords.csv\", index=False)\n",
    "print(\"Preprocessed dataset saved as 'preprocessed_passwords.csv'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46668748-0428-499d-95ad-08d2df5d6e21",
   "metadata": {},
   "source": [
    "# II. Data Preprocessing for LSTM Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1fcb10-4e9c-4723-9e8d-e3fed6c15927",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install tensorflow[and-cuda]\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26500cf3-1f06-4395-82f2-e8d63cd7bc94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load preprocessed dataset\n",
    "df = pd.read_csv(\"preprocessed_passwords.csv\", dtype=str, low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd24afcb-9c31-46e5-b9bf-89acbb48dfea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop NaN values (they cause tokenizer errors)\n",
    "df = df.dropna(subset=[\"Password\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f042a862-9d7a-4c6e-be4b-a355549304ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert password strength labels to numerical values\n",
    "label_encoder = LabelEncoder()\n",
    "df[\"Strength\"] = label_encoder.fit_transform(df[\"Strength\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76bb7490-883e-4125-aa32-082d22c94b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure all passwords are strings\n",
    "df[\"Password\"] = df[\"Password\"].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e077a21-1622-4296-87a0-834da249d14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer with limited vocab size to reduce memory usage\n",
    "tokenizer = Tokenizer(char_level=True, num_words=10000)  # Limit vocab size to optimize memory\n",
    "tokenizer.fit_on_texts(df[\"Password\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f206eee-b37f-4b48-8424-e4c3612925db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert passwords to sequences\n",
    "sequences = tokenizer.texts_to_sequences(df[\"Password\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e885fc3-d151-4000-a505-b0aad5c9e2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad sequences dynamically based on 95th percentile length to avoid long padding\n",
    "max_length = int(np.percentile([len(seq) for seq in sequences], 95))  # Avoid extreme long passwords\n",
    "X = pad_sequences(sequences, maxlen=max_length, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f72004c-9f71-4d5d-bd6b-34184132fc51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert labels to numpy array\n",
    "y = np.array(df[\"Strength\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89cb04ce-b2b4-4b75-9901-073d59cbaae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Training set size: {len(X_train)}\")\n",
    "print(f\"Test set size: {len(X_test)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad658ed-fa56-48c4-bdbe-a7451320ddd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save preprocessed data\n",
    "np.save(\"X_train.npy\", X_train)\n",
    "np.save(\"X_test.npy\", X_test)\n",
    "np.save(\"y_train.npy\", y_train)\n",
    "np.save(\"y_test.npy\", y_test)\n",
    "print(\"Preprocessed data saved for LSTM training.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51675971-e831-4940-a82b-df693c63be64",
   "metadata": {},
   "source": [
    "# III. Building & Training the LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0b6192-736d-4a7f-bbf6-9a576c97cd69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the LSTM model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(input_dim=10000, output_dim=128),\n",
    "    tf.keras.layers.LSTM(64, return_sequences=True),\n",
    "    tf.keras.layers.LSTM(32),\n",
    "    tf.keras.layers.Dense(16, activation='relu'),\n",
    "    tf.keras.layers.Dense(3, activation='softmax')  # 3 classes: Weak, Moderate, Strong\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1c4441-cc29-45d2-9bcc-2d0618c76327",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=3, batch_size=64, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c996a0dd-e974-4c33-a61f-07d16d7b02af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "model.save(\"password_strength_lstm.h5\")\n",
    "print(\"LSTM model trained and saved successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c403cb-50a2-43c1-9dbc-dd584a1e8503",
   "metadata": {},
   "source": [
    "# IV. Password Strength UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef656fb-f785-4f02-9a92-05ab1bbe8ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "431354ce-68c7-4717-a669-b0993f0092b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting fastapi[all]\n",
      "  Downloading fastapi-0.115.11-py3-none-any.whl.metadata (27 kB)\n",
      "Collecting starlette<0.47.0,>=0.40.0 (from fastapi[all])\n",
      "  Downloading starlette-0.46.1-py3-none-any.whl.metadata (6.2 kB)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in /home/technomind/anaconda3/lib/python3.11/site-packages (from fastapi[all]) (2.10.6)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/technomind/anaconda3/lib/python3.11/site-packages (from fastapi[all]) (4.12.2)\n",
      "Collecting fastapi-cli>=0.0.5 (from fastapi-cli[standard]>=0.0.5; extra == \"all\"->fastapi[all])\n",
      "  Downloading fastapi_cli-0.0.7-py3-none-any.whl.metadata (6.2 kB)\n",
      "Requirement already satisfied: httpx>=0.23.0 in /home/technomind/anaconda3/lib/python3.11/site-packages (from fastapi[all]) (0.26.0)\n",
      "Collecting jinja2>=3.1.5 (from fastapi[all])\n",
      "  Using cached jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting python-multipart>=0.0.18 (from fastapi[all])\n",
      "  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: itsdangerous>=1.1.0 in /home/technomind/anaconda3/lib/python3.11/site-packages (from fastapi[all]) (2.0.1)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in /home/technomind/anaconda3/lib/python3.11/site-packages (from fastapi[all]) (6.0.1)\n",
      "Requirement already satisfied: ujson!=4.0.2,!=4.1.0,!=4.2.0,!=4.3.0,!=5.0.0,!=5.1.0,>=4.0.1 in /home/technomind/anaconda3/lib/python3.11/site-packages (from fastapi[all]) (5.4.0)\n",
      "Collecting orjson>=3.2.1 (from fastapi[all])\n",
      "  Downloading orjson-3.10.15-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (41 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m41.8/41.8 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting email-validator>=2.0.0 (from fastapi[all])\n",
      "  Downloading email_validator-2.2.0-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting uvicorn>=0.12.0 (from uvicorn[standard]>=0.12.0; extra == \"all\"->fastapi[all])\n",
      "  Downloading uvicorn-0.34.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting pydantic-settings>=2.0.0 (from fastapi[all])\n",
      "  Downloading pydantic_settings-2.8.1-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting pydantic-extra-types>=2.0.0 (from fastapi[all])\n",
      "  Downloading pydantic_extra_types-2.10.3-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting dnspython>=2.0.0 (from email-validator>=2.0.0->fastapi[all])\n",
      "  Downloading dnspython-2.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: idna>=2.0.0 in /home/technomind/anaconda3/lib/python3.11/site-packages (from email-validator>=2.0.0->fastapi[all]) (3.4)\n",
      "Collecting typer>=0.12.3 (from fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"all\"->fastapi[all])\n",
      "  Downloading typer-0.15.2-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting rich-toolkit>=0.11.1 (from fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"all\"->fastapi[all])\n",
      "  Downloading rich_toolkit-0.13.2-py3-none-any.whl.metadata (999 bytes)\n",
      "Requirement already satisfied: anyio in /home/technomind/anaconda3/lib/python3.11/site-packages (from httpx>=0.23.0->fastapi[all]) (4.2.0)\n",
      "Requirement already satisfied: certifi in /home/technomind/anaconda3/lib/python3.11/site-packages (from httpx>=0.23.0->fastapi[all]) (2024.2.2)\n",
      "Requirement already satisfied: httpcore==1.* in /home/technomind/anaconda3/lib/python3.11/site-packages (from httpx>=0.23.0->fastapi[all]) (1.0.7)\n",
      "Requirement already satisfied: sniffio in /home/technomind/anaconda3/lib/python3.11/site-packages (from httpx>=0.23.0->fastapi[all]) (1.3.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /home/technomind/anaconda3/lib/python3.11/site-packages (from httpcore==1.*->httpx>=0.23.0->fastapi[all]) (0.14.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/technomind/anaconda3/lib/python3.11/site-packages (from jinja2>=3.1.5->fastapi[all]) (2.1.3)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/technomind/anaconda3/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi[all]) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /home/technomind/anaconda3/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi[all]) (2.27.2)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in /home/technomind/anaconda3/lib/python3.11/site-packages (from pydantic-settings>=2.0.0->fastapi[all]) (0.21.0)\n",
      "Requirement already satisfied: click>=7.0 in /home/technomind/anaconda3/lib/python3.11/site-packages (from uvicorn>=0.12.0->uvicorn[standard]>=0.12.0; extra == \"all\"->fastapi[all]) (8.1.7)\n",
      "Collecting httptools>=0.6.3 (from uvicorn[standard]>=0.12.0; extra == \"all\"->fastapi[all])\n",
      "  Downloading httptools-0.6.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
      "Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn[standard]>=0.12.0; extra == \"all\"->fastapi[all])\n",
      "  Downloading uvloop-0.21.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.12.0; extra == \"all\"->fastapi[all])\n",
      "  Downloading watchfiles-1.0.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "Collecting websockets>=10.4 (from uvicorn[standard]>=0.12.0; extra == \"all\"->fastapi[all])\n",
      "  Downloading websockets-15.0.1-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting rich>=13.7.1 (from rich-toolkit>=0.11.1->fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"all\"->fastapi[all])\n",
      "  Using cached rich-13.9.4-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting shellingham>=1.3.0 (from typer>=0.12.3->fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"all\"->fastapi[all])\n",
      "  Downloading shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/technomind/anaconda3/lib/python3.11/site-packages (from rich>=13.7.1->rich-toolkit>=0.11.1->fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"all\"->fastapi[all]) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/technomind/anaconda3/lib/python3.11/site-packages (from rich>=13.7.1->rich-toolkit>=0.11.1->fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"all\"->fastapi[all]) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/technomind/anaconda3/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich>=13.7.1->rich-toolkit>=0.11.1->fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"all\"->fastapi[all]) (0.1.0)\n",
      "Downloading email_validator-2.2.0-py3-none-any.whl (33 kB)\n",
      "Downloading fastapi_cli-0.0.7-py3-none-any.whl (10 kB)\n",
      "Using cached jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
      "Downloading orjson-3.10.15-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (130 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m130.3/130.3 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pydantic_extra_types-2.10.3-py3-none-any.whl (37 kB)\n",
      "Downloading pydantic_settings-2.8.1-py3-none-any.whl (30 kB)\n",
      "Downloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
      "Downloading starlette-0.46.1-py3-none-any.whl (71 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading uvicorn-0.34.0-py3-none-any.whl (62 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading fastapi-0.115.11-py3-none-any.whl (94 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m94.9/94.9 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading dnspython-2.7.0-py3-none-any.whl (313 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m313.6/313.6 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading httptools-0.6.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (459 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m459.8/459.8 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading rich_toolkit-0.13.2-py3-none-any.whl (13 kB)\n",
      "Downloading typer-0.15.2-py3-none-any.whl (45 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m45.1/45.1 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading uvloop-0.21.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.0 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading watchfiles-1.0.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (452 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m452.6/452.6 kB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading websockets-15.0.1-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (182 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m182.3/182.3 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached rich-13.9.4-py3-none-any.whl (242 kB)\n",
      "Downloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Installing collected packages: websockets, uvloop, uvicorn, shellingham, python-multipart, orjson, jinja2, httptools, dnspython, watchfiles, starlette, rich, email-validator, typer, rich-toolkit, pydantic-settings, pydantic-extra-types, fastapi, fastapi-cli\n",
      "  Attempting uninstall: jinja2\n",
      "    Found existing installation: Jinja2 3.1.3\n",
      "    Uninstalling Jinja2-3.1.3:\n",
      "      Successfully uninstalled Jinja2-3.1.3\n",
      "  Attempting uninstall: rich\n",
      "    Found existing installation: rich 13.3.5\n",
      "    Uninstalling rich-13.3.5:\n",
      "      Successfully uninstalled rich-13.3.5\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torch 2.6.0 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
      "torch 2.6.0 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
      "torch 2.6.0 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
      "torch 2.6.0 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
      "torch 2.6.0 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
      "torch 2.6.0 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
      "torch 2.6.0 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
      "torch 2.6.0 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
      "torch 2.6.0 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
      "torch 2.6.0 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed dnspython-2.7.0 email-validator-2.2.0 fastapi-0.115.11 fastapi-cli-0.0.7 httptools-0.6.4 jinja2-3.1.6 orjson-3.10.15 pydantic-extra-types-2.10.3 pydantic-settings-2.8.1 python-multipart-0.0.20 rich-13.9.4 rich-toolkit-0.13.2 shellingham-1.5.4 starlette-0.46.1 typer-0.15.2 uvicorn-0.34.0 uvloop-0.21.0 watchfiles-1.0.4 websockets-15.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip install \"fastapi[all]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f2802be5-1dde-4e30-b3ad-6170f39a2bde",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-22 07:52:22.229851: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1742626342.302546  178134 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1742626342.323278  178134 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-03-22 07:52:22.484121: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-03-22 07:52:25.461313: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:152] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "from fastapi import FastAPI\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pickle\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "# Load model and tokenizer\n",
    "model = tf.keras.models.load_model(\"password_strength_lstm.h5\")\n",
    "with open(\"tokenizer.pkl\", \"rb\") as handle:\n",
    "    tokenizer = pickle.load(handle)\n",
    "\n",
    "max_length = 50  # Must match training\n",
    "\n",
    "def predict_strength(password):\n",
    "    sequence = tokenizer.texts_to_sequences([password])\n",
    "    padded_sequence = pad_sequences(sequence, maxlen=max_length, padding='post')\n",
    "    prediction = model.predict(padded_sequence)\n",
    "    strength_labels = [\"Weak\", \"Moderate\", \"Strong\"]\n",
    "    return {\"strength\": strength_labels[np.argmax(prediction)], \"confidence\": float(np.max(prediction))}\n",
    "\n",
    "@app.get(\"/\")\n",
    "def home():\n",
    "    return {\"message\": \"FastAPI is running successfully!\"}\n",
    "    \n",
    "@app.get(\"/predict/\")\n",
    "def get_strength(password: str):\n",
    "    return predict_strength(password)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8275b3b-c80c-4389-ac11-bff620161c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recompile the model (if needed)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9a6569-13ea-432d-9ec8-14feeced93d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained model\n",
    "model = tf.keras.models.load_model(\"password_strength_lstm.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647f8c46-8894-4f8b-b269-15aca920f726",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer\n",
    "\n",
    "with open(\"tokenizer.pkl\", \"wb\") as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501ea81c-910a-4f97-baad-a9a723a75117",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define max_length (must match training configuration)\n",
    "max_length = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f19fe676-dbc9-4e56-ba2c-925c8b671977",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to predict password strength\n",
    "def predict_strength(password):\n",
    "    st.write(\"Debug: Checking if model and tokenizer are loaded.\")\n",
    "    sequence = tokenizer.texts_to_sequences([password])\n",
    "    padded_sequence = pad_sequences(sequence, maxlen=max_length, padding='post')\n",
    "    prediction = model.predict(padded_sequence)\n",
    "    strength_labels = [\"Weak\", \"Moderate\", \"Strong\"]\n",
    "    return strength_labels[np.argmax(prediction)], np.max(prediction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a060494-a18c-47bf-bf0d-53cad4d9b8b6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'st' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 32\u001b[0m\n\u001b[1;32m     29\u001b[0m             st\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease enter a password.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 32\u001b[0m     main()\n",
      "Cell \u001b[0;32mIn[1], line 4\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmain\u001b[39m():\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;66;03m# Debugging logs\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m     st\u001b[38;5;241m.\u001b[39mset_page_config(layout\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwide\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m     st\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124müîç Checking model and tokenizer loading...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'st' is not defined"
     ]
    }
   ],
   "source": [
    "# Streamlit UI\n",
    "def main():\n",
    "    # Debugging logs\n",
    "    st.set_page_config(layout=\"wide\")\n",
    "    st.write(\"üîç Checking model and tokenizer loading...\")\n",
    "\n",
    "    try:\n",
    "        model = tf.keras.models.load_model(\"password_strength_lstm.h5\")\n",
    "        st.write(\"‚úÖ Model loaded successfully.\")\n",
    "    except Exception as e:\n",
    "        st.error(f\"‚ùå Model loading failed: {e}\")\n",
    "    \n",
    "    try:\n",
    "        with open(\"tokenizer.pkl\", \"rb\") as handle:\n",
    "            tokenizer = pickle.load(handle)\n",
    "        st.write(\"‚úÖ Tokenizer loaded successfully.\")\n",
    "    except Exception as e:\n",
    "        st.error(f\"‚ùå Tokenizer loading failed: {e}\")\n",
    "\n",
    "    st.title(\"üîê AI-Based Password Strength Analyzer\")\n",
    "    st.write(\"Enter a password to check its strength using an AI model.\")\n",
    "    \n",
    "    password = st.text_input(\"Enter Password:\", type=\"password\")\n",
    "    if st.button(\"Check Strength\"):\n",
    "        if password:\n",
    "            strength, confidence = predict_strength(password)\n",
    "            st.success(f\"Password Strength: {strength} (Confidence: {confidence:.2f})\")\n",
    "        else:\n",
    "            st.warning(\"Please enter a password.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77dca807-9f47-47b6-9d08-b43c0e511684",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
